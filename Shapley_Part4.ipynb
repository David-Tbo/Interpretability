{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance de Shapley ?  \n",
    "\n",
    "L'utilisation de Shapley values pour expliquer les prédictions d'un modèle complexe est une méthode puissante pour rendre les modèles interprétables. Cependant, évaluer la performance de Shapley values dans ce contexte peut être un peu plus subtil que d'évaluer la performance d'un modèle de machine learning traditionnel. Voici quelques approches pour évaluer la performance de Shapley values :\n",
    "\n",
    "### 1. **Consistance avec les attentes**\n",
    "Vérifiez si les Shapley values correspondent à vos attentes basées sur votre connaissance du domaine. Par exemple, si vous savez qu'une certaine caractéristique devrait avoir un impact important sur la prédiction, les Shapley values devraient refléter cela.\n",
    "\n",
    "### 2. **Comparaison avec d'autres méthodes d'explication**\n",
    "Comparez les Shapley values avec d'autres méthodes d'explication de modèles, comme LIME (Local Interpretable Model-agnostic Explanations) ou les poids des caractéristiques dans des modèles plus simples. Si les explications sont cohérentes entre différentes méthodes, cela renforce la confiance dans les Shapley values.\n",
    "\n",
    "### 3. **Évaluation quantitative**\n",
    "Utilisez des métriques quantitatives pour évaluer la performance des Shapley values. Voici quelques métriques couramment utilisées :\n",
    "\n",
    "#### a. **Fidélité**\n",
    "Mesurez la fidélité des explications en comparant les prédictions du modèle original avec les prédictions d'un modèle simplifié basé sur les Shapley values. Une bonne fidélité signifie que les explications sont proches des prédictions réelles du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "# Supposons que vous avez un modèle 'model' et des données 'X'\n",
    "explainer = shap.Explainer(model, X)\n",
    "shap_values = explainer(X)\n",
    "\n",
    "# Calculer la fidélité\n",
    "def fidelity(model, X, shap_values):\n",
    "    original_predictions = model.predict(X)\n",
    "    shap_predictions = np.sum(shap_values.values, axis=1) + shap_values.base_values\n",
    "    return np.mean((original_predictions - shap_predictions) ** 2)\n",
    "\n",
    "fidelity_score = fidelity(model, X, shap_values)\n",
    "print(f\"Fidelity score: {fidelity_score}\")\n",
    "```\n",
    "\n",
    "#### b. **Stabilité**\n",
    "Mesurez la stabilité des Shapley values en calculant les valeurs pour des échantillons légèrement perturbés. Les Shapley values devraient être relativement stables pour des échantillons similaires.\n",
    "\n",
    "```python\n",
    "def stability(model, X, shap_values, perturbation_size=0.01):\n",
    "    perturbed_X = X + np.random.normal(0, perturbation_size, X.shape)\n",
    "    perturbed_shap_values = explainer(perturbed_X)\n",
    "    return np.mean((shap_values.values - perturbed_shap_values.values) ** 2)\n",
    "\n",
    "stability_score = stability(model, X, shap_values)\n",
    "print(f\"Stability score: {stability_score}\")\n",
    "```\n",
    "\n",
    "### 4. **Évaluation qualitative**\n",
    "Impliquez des experts du domaine pour évaluer qualitativement les explications fournies par les Shapley values. Les experts peuvent fournir des insights sur la pertinence et la précision des explications.\n",
    "\n",
    "### 5. **Visualisation**\n",
    "Utilisez des visualisations pour inspecter les Shapley values. Les graphiques de dépendance, les graphiques de résumé et les graphiques de force peuvent aider à comprendre l'impact des différentes caractéristiques sur les prédictions.\n",
    "\n",
    "```python\n",
    "# Graphique de résumé\n",
    "shap.summary_plot(shap_values, X)\n",
    "\n",
    "# Graphique de dépendance\n",
    "shap.dependence_plot(\"feature_name\", shap_values.values, X)\n",
    "\n",
    "# Graphique de force\n",
    "shap.force_plot(explainer.expected_value, shap_values.values, X)\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "Évaluer la performance des Shapley values dans le contexte de l'interprétabilité des modèles nécessite une combinaison de méthodes quantitatives et qualitatives. En utilisant des métriques comme la fidélité et la stabilité, en comparant avec d'autres méthodes d'explication, et en impliquant des experts du domaine, vous pouvez obtenir une évaluation complète de la performance des Shapley values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting bias with SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting bias with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "gender_cols = [col for col in X_train.columns if 'Gender' in col] + [\"Trans\"]\n",
    "\n",
    "gender_features_locs = [X_train.columns.get_loc(col) for col in gender_cols]\n",
    "\n",
    "def describe_stats(gender_col):\n",
    "    s = stats.describe(shap_values[X_train[gender_col]][:,gender_features_locs].sum(axis=1))\n",
    "    return (gender_col, int(s.nobs), float(s.minmax[0]), float(s.minmax[1]), float(s.mean))\n",
    "\n",
    "display(pd.DataFrame([describe_stats(col) for col in gender_cols], [\"gender_col\", \"n\", \"min\", \"max\", \"mean\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
